60,62c60,62
<         self.to_q_weight = nn.Parameter(shape=[inner_dim, query_dim], dtype=dtype)
<         self.to_k_weight = nn.Parameter(shape=[inner_dim, context_dim], dtype=dtype)
<         self.to_v_weight = nn.Parameter(shape=[inner_dim, context_dim], dtype=dtype)
---
>         self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
>         self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
>         self.to_v = nn.Linear(context_dim, inner_dim, bias=False)
71,76c71
<         layout = "20314" if USE_CUDA else "m2n3"
< 
<         bs, seqlen, _ = get_shape(x)
<         q = ops.gemm_rcr_permute(shape=(seqlen, 1, nheads), layout=layout)(  # type: ignore
<             ops.reshape()(x, [bs * seqlen, -1]), self.to_q_weight.tensor()
<         )
---
>         q = self.to_q(x)
77a73,74
>         k = self.to_k(context)
>         v = self.to_v(context)
79,85c76
<         seqlen = get_shape(context)[1]
<         k = ops.gemm_rcr_permute(shape=(seqlen, 1, nheads), layout=layout)(  # type: ignore
<             ops.reshape()(context, [bs * seqlen, -1]), self.to_k_weight.tensor()  # type: ignore
<         )
<         v = ops.gemm_rcr_permute(shape=(seqlen, 1, nheads), layout=layout)(  # type: ignore
<             ops.reshape()(context, [bs * seqlen, -1]), self.to_v_weight.tensor()  # type: ignore
<         )
---
>         bs = q.shape()[0]
87,100c78,90
<         if USE_CUDA:
<             attn_op = ops.mem_eff_attention(causal=False)
<             out = attn_op(
<                 (ops.reshape()(q, [bs, nheads, -1, d])),
<                 (ops.reshape()(k, [bs, nheads, -1, d])),
<                 (ops.reshape()(v, [bs, nheads, -1, d])),
<             )
<         else:
<             OP = ops.bmm_softmax_bmm_permute(shape=(nheads,), scale=self.scale)
<             out = OP(
<                 (ops.reshape()(q, [bs * nheads, -1, d])),
<                 (ops.reshape()(k, [bs * nheads, -1, d])),
<                 (ops.reshape()(v, [bs * nheads, -1, d])),
<             )
---
>         q = ops.reshape()(q, [bs, -1, self.heads, self.dim_head])
>         k = ops.reshape()(k, [bs, -1, self.heads, self.dim_head])
>         v = ops.reshape()(v, [bs, -1, self.heads, self.dim_head])
>         q = ops.permute()(q, [0, 2, 1, 3])
>         k = ops.permute()(k, [0, 2, 1, 3])
>         v = ops.permute()(v, [0, 2, 1, 3])
> 
>         attn_op = ops.mem_eff_attention(causal=False)
>         out = attn_op(
>             (ops.reshape()(q, [bs, nheads, -1, d])),
>             (ops.reshape()(k, [bs, nheads, -1, d])),
>             (ops.reshape()(v, [bs, nheads, -1, d])),
>         )
198c188,195
<         self, in_channels, n_heads, d_head, depth=1, dropout=0.0, context_dim=None
---
>         self,
>         in_channels,
>         n_heads,
>         d_head,
>         depth=1,
>         dropout=0.0,
>         context_dim=None,
>         use_linear_projection=False,
203a201
>         self.use_linear_projection = use_linear_projection
205,207c203,208
<         self.proj_in = nn.Conv2dBias(
<             in_channels, inner_dim, kernel_size=1, stride=1, padding=0
<         )
---
>         if use_linear_projection:
>             self.proj_in = nn.Linear(in_channels, inner_dim)
>         else:
>             self.proj_in = nn.Conv2dBias(
>                 in_channels, inner_dim, kernel_size=1, stride=1, padding=0
>             )
218,220c219,224
<         self.proj_out = nn.Conv2dBias(
<             inner_dim, in_channels, kernel_size=1, stride=1, padding=0
<         )
---
>         if use_linear_projection:
>             self.proj_out = nn.Linear(inner_dim, in_channels)
>         else:
>             self.proj_out = nn.Conv2dBias(
>                 inner_dim, in_channels, kernel_size=1, stride=1, padding=0
>             )
224c228
<         b, h, w, c = get_shape(x)
---
>         b, h, w, c = x.shape()
227,228c231,237
<         x = self.proj_in(x)
<         x = ops.reshape()(x, [b, -1, c])
---
>         if self.use_linear_projection:
>             x = ops.reshape()(x, [b, -1, c])
>             x = self.proj_in(x)
>         else:
>             x = self.proj_in(x)
>             x = ops.reshape()(x, [b, -1, c])
> 
231,232c240,246
<         x = ops.reshape()(x, [b, h, w, c])
<         x = self.proj_out(x)
---
> 
>         if self.use_linear_projection:
>             x = self.proj_out(x)
>             x = ops.reshape()(x, [b, h, w, c])
>         else:
>             x = ops.reshape()(x, [b, h, w, c])
>             x = self.proj_out(x)
315c329
<         shape = get_shape(x)
---
>         shape = x.shape()
343c357
<         shape = get_shape(x)
---
>         # shape = get_shape(x)
347c361
<         return ops.reshape()(x, shape)
---
>         return ops.reshape()(x, x.shape())
370,374c384,388
<         self.self_attn = nn.MultiheadAttention(
<             dim=hidden_size,
<             batch_size=batch_size,
<             seq_len=seq_len,
<             num_heads=num_attention_heads,
---
>         self.self_attn = nn.CrossAttention(
>             hidden_size,
>             seq_len,
>             seq_len,
>             num_attention_heads,
376,378d389
<             attn_drop=attention_dropout,
<             proj_drop=0,
<             has_residual=True,
380,381d390
<             mask_seq=mask_seq,
<             use_mem_eff=True,
382a392
> 
407c417,419
<         hidden_states = self.self_attn(hidden_states, residual)
---
>         hidden_states = self.self_attn(
>             hidden_states, hidden_states, hidden_states, residual
>         )
525a538,540
>         self.max_position_embeddings = max_position_embeddings
>         self.embed_dim = hidden_size
>         self.vocab_size = vocab_size
541,543c556,560
<         input_ids = ops.reshape()(input_ids, [-1])
< 
<         position_ids = ops.reshape()(position_ids, [-1])
---
>         token_embedding = self.token_embedding.tensor()
>         token_embedding = ops.reshape()(
>             token_embedding, [1, self.vocab_size, self.embed_dim]
>         )
>         token_embedding = ops.expand()(token_embedding, [input_shape[0], -1, -1])  # type: ignore
546c563
<             inputs_embeds = ops.batch_gather()(self.token_embedding.tensor(), input_ids)
---
>             inputs_embeds = ops.batch_gather()(token_embedding, input_ids)
548,549c565,567
<         position_embeddings = ops.batch_gather()(
<             self.position_embedding.tensor(), position_ids
---
>         position_embedding = self.position_embedding.tensor()
>         position_embedding = ops.reshape()(
>             position_embedding, [1, self.max_position_embeddings, self.embed_dim]
550a569,571
>         position_embedding = ops.expand()(position_embedding, [input_shape[0], -1, -1])  # type: ignore
> 
>         position_embeddings = ops.batch_gather()(position_embedding, position_ids)
